% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/contributions_model.R
\docType{data}
\name{contributions_model}
\alias{contributions_model}
\alias{contributions_dataset}
\alias{read.contributions_model}
\alias{train.contributions_model}
\alias{predict.contributions_model}
\alias{output.contributions_model}
\title{contributions_model}
\format{
An object of class \code{contributions_model} (inherits from \code{mlr_report}, \code{report}, \code{list}) of length 0.
}
\usage{
contributions_model

contributions_dataset(
  since = Sys.Date() - 365 * 5,
  until = Sys.Date(),
  rebuild_dataset = NULL,
  chunk_size = 1e+07,
  ...
)

\method{read}{contributions_model}(
  model,
  since = Sys.Date() - 365 * 5,
  until = Sys.Date(),
  predict_since = Sys.Date() - 30,
  rebuild_dataset = NULL,
  downsample_read = 1,
  predict = NULL,
  ...
)

\method{train}{contributions_model}(model, num_trees = 512, downsample_train = 1, ...)

\method{predict}{contributions_model}(model, ...)

\method{output}{contributions_model}(
  model,
  downsample_output = 1,
  features = NULL,
  n_repetitions = 5,
  n_top = 500,
  n_features = 25,
  ...
)
}
\arguments{
\item{since}{Date/POSIXct data on or after this date will be loaded and possibly used for training}

\item{until}{Date/POSIXct data after this date will not be used for training or predictions, defaults to the beginning of today}

\item{rebuild_dataset}{boolean rebuild the dataset by calling \code{contributions_dataset(since=since,until=until)} (TRUE), just read the existing one (FALSE),
or append new rows by calling \code{contributions_dataset(since=max_existing_date,until=until)} (NULL, default)}

\item{chunk_size}{integer(1) number of rows per partition}

\item{...}{not used}

\item{model}{\code{contributions_model} object}

\item{predict_since}{Date/POSIXct data on/after this date will be used to make predictions and not for training}

\item{downsample_read}{\code{numeric(1)} the amount to downsample the dataset on read}

\item{predict}{Not used, just here to prevent partial argument matching}

\item{num_trees}{\code{integer(1)} maximum number of trees to use for ranger model}

\item{downsample_train}{\code{double(1)} fraction of observations to use for training, defaults to .1}

\item{downsample_output}{\code{numeric(1)} the proportion of the test set to use for feature importance and
Shapley explanations}

\item{features}{\code{character} The names of the features for which to compute the feature effects/importance.}

\item{n_repetitions}{\code{numeric(1)} How many shufflings of the features should be done? See \link[iml:FeatureImp]{iml::FeatureImp} for more info.}

\item{n_top}{\code{integer(1)} the number of rows, ranked by probability to analyze and explain as 'top picks'.}

\item{n_features}{\code{integer(1)} the number of features, ranked by importance to analyze.}
}
\description{
mlr3 model for predicting a customer's first contribution
}
\section{Methods (by generic)}{
\itemize{
\item \code{read(contributions_model)}: Read in contribution data and prepare a mlr3 training task and a prediction/validation task

\item \code{train(contributions_model)}: Tune and train a stacked log-reg/ranger model on the data

\item \code{predict(contributions_model)}: Predict using the trained model

\item \code{output(contributions_model)}: create IML reports for contributions_model

}}
\section{Functions}{
\itemize{
\item \code{contributions_dataset()}: Build the contributions dataset from the overall stream.
\itemize{
\item events are the first contribution per household > $50
\item data after an event are censored
\item contribution indicators are rolled back and timestamps are normalized to the start of customer activity
\item only data since \code{since} are loaded
Data is written to the primary cache partitioned by year and then synced across storages
}

}}
\note{
Data will be loaded in-memory, because \emph{[inaudible]} mlr3 doesn't work well with factors encoded as dictionaries in arrow tables.
}
\section{Preprocessing:}{
\itemize{
\item ignore 1-day and email "send" features because they leak data
\item remove constant features
\item balance classes to a ratio of 1:10 T:F
\item Yeo-Johnson with tuned boundaries
\item impute missing values out-of-range and add missing data indicator features
\item feature importance filter (for log-reg submodel only)
}
}

\section{Model:}{
\itemize{
\item stacked log-reg + ranger > log-reg model
\item tuned using a hyperband method on the AUC (sensitivity/specificity)
}
}

\keyword{datasets}
